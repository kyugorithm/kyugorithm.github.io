

<html>
<head>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css">
  <meta charset="utf-8">
  <meta name="description" content="Toward Temporally Consistent Video Face Re-Aging: A Benchmark dataset and Quantitative Metrics">
  <meta name="keywords" content="Face Re-Aging, GAN">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Toward Temporally Consistent Video Face Re-Aging: A Benchmark dataset and Quantitative Metrics</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Toward Temporally Consistent Video Face Re-Aging: A Benchmark dataset and Quantitative Metrics</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Abdul Muqeet</a><sup>1</sup>,
            </span>
	    <span class="author-block">
              <a href="https://kyugorithm.github.io">Kyuchul Lee</a><sup>1</sup>,</span>
            <span class="author-block">
              Beomsoo Kim</a><sup>2</sup>,
            </span>
            <span class="author-block">
              Yohan Hong</a><sup>2</sup>,
            </span>
	   <span class="author-block">
              Hyunrae Lee</a><sup>2</sup>,
            </span>
	   <span class="author-block">
              Woongon Kim</a><sup>2</sup>,
            </span>
	   <span class="author-block">
              Hwihun Oh</a><sup>2</sup>,
            </span>
            <br>
            <span class="author-block">
              KwangHee Lee</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>VIVE STUDIOS,</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/xxxx.xxxxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/kyugorithm/xxxxxxxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.jpg"
                 class="teaser-image"
                 alt="teaser image."/>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">DiffFace</span> gradually produces images with source identity and target attributes such as gaze, structure and pose
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Face age transformation or face re-aging, the process of manipulating the apparent age in facial images, has attracted the attention of researchers due to its various potential applications in digital editing, forensics, and security. Despite promising results from recent studies, translating this technology into video applications remains challenging. This is because existing state-of-the-art methods process each image individually and do not guarantee temporal consistency. On the other hand, various video editing methods manipulate the age in the latent space, displaying improved temporal consistencies, but their age transformation quality needs to be improved. To address these limitations, we present the first synthetic video dataset that includes videos of subjects at varying paired ages. Our proposed dataset leverages recent advances in face reenactment and frame interpolation to synthesize diverse poses, expressions, and natural motion. To demonstrate the quality of the proposed dataset, we introduce the Video Face Re-Aging Network, a video age transformation solution that effectively solves the consistency problem. We utilize multiple discriminators that consider both temporal consistency and age transformation, preserving the quality of frames without affecting the temporal consistency across the frames. We propose two novel metrics to assess the model's translation equivariance and the temporal consistency of age-related regions. We compare our method with existing state-of-the-art methods on publicly available image and video datasets using both qualitative and quantitative experiments. Experimental results show that our method outperforms the existing approaches in age transformation and video consistency.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">ID Conditional DDPM</h2>
          <p>
            We employ the structure of the conditional diffusion model, where source identity information can be injected.
            At the same time, we propose
            an identity loss for the diffusion model to preserve the facial
            identity effectively
          </p>
          <img src="./static/images/training.jpg"
               class="training-image"
               alt="training image."/>
        </div>
      </div>
      <!--/ Training ID Conditional DDPM. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Algorithm</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              The overall process for training ID Conditional DDPM. We verify the benefit of the ID Conditional DDPM in the paper
            </p>
            <img src="./static/images/trainingAlg.jpg"
                 class="trainingAlg-image"
                 alt="trainingAlg image."/>
          </div>
        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Sampling</h2>
        <img src="./static/images/sampling.jpg"
             class="training-image"
             alt="training image."/>

        <div class="content has-text-justified">
          <p>
            To control the facial attributes of generated images, we
            propose facial guidance that is applied during the diffusion
            process. One major advantage of using the diffusion model
            is that once the model is trained, it can control the image
            driven by the guidance during the sampling process. Thus
            we can obtain desired images without any re-training of
            the diffusion model. In order to utilize this advantage, we
            give facial guidance using external models, such as identity
            embedder, face parser, and gaze estimator during the sampling process. Note that we can use any offthe-shelf facial model, and they can be adaptively
            selected according to the userâ€™s purpose.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Target Preserving Blending </h2>
        <img src="./static/images/sampling_progress.png"
             class="sampling_progress"
             alt="sampling_progress image."/>

        <div class="content has-text-justified">
          <p>
            Target preserving
            blending method that alters the mask intensity to better preserve structural attributes of target. Target preserving blending is to gradually increase the mask intensity from zero to
            one, according to the time of the diffusion process T. By
            adjusting the starting point where the intensity of the mask
            becomes one, we can adaptively maintain the structure of the
            target image
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Comparison Results</h2>
        <img src="./static/images/exp_compare_result.jpg"
             class="training-image"
             alt="training image."/>

        <div class="content has-text-justified">
          <p>
            Our DiffFace outperforms other models
            in terms of changing identity-related attributes. For example,
            in the first and fourth rows, we notice our result reflects more
            vivid lips and eyes, while other results models tend to have
            eyes and lip colors from target images. This shows that our
            model more effectively transfers identity-related attributes
            than other models.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Out-of-Domain Face Swapping Results</h2>
        <img src="./static/images/sampling_ood.png"
             class="training-image"
             alt="training image."/>
        <div class="content has-text-justified">
          <p>
            We provide additional collections of swapped-image samples based on our DiffFace model.
            Although our model is not trained on any of these oil portrait paintings, the results reflects the characteristics of each domain with shape changing.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{kim2022diffface,
  author    = {K. Kim, Y. Kim, S. Cho, J. Seo, J. Nam, K. Lee, S. Kim, K. Lee},
  title     = {DiffFace: Diffusion-based Face Swapping with Facial Guidance},
  journal   = {Arxiv},
  year      = {2022},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
